# ğŸ§  MultiAgent Githubify

An AI-powered assistant that lets you ask questions about your codebase and get smart, summarized answers using LLMs (Large Language Models)!

Supports both **Terminal Client** and **Streamlit Web UI** ğŸš€

---

## ğŸ“‚ Project Structure

```plaintext
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ retriever_agent.py   # Retrieves relevant code snippets
â”‚   â””â”€â”€ reasoner_agent.py    # Summarizes snippets using LLM
â”œâ”€â”€ client/
â”‚   â””â”€â”€ chat_client.py       # Terminal-based chat client
â”œâ”€â”€ server/
â”‚   â””â”€â”€ main.py              # FastAPI WebSocket server (MCP)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ builder.py           # Builds vectorstore (FAISS index)
â”‚   â””â”€â”€ embedder.py          # Embedding code using transformer model
â”œâ”€â”€ vectorstore/             # FAISS index and documents
â”‚   â”œâ”€â”€ faiss_index.bin
â”‚   â””â”€â”€ docs.pkl
â”œâ”€â”€ streamlit_chat_client.py # Streamlit-based frontend UI
â”œâ”€â”€ .env                     # Environment variables (API keys)
â””â”€â”€ README.md                # (this file)
```

---

## ğŸš€ How to Setup & Run Locally

### 1. Clone the Repository
```bash
git clone https://github.com/Aniket-Suthar/multiagent_githubify.git
cd multiagent_githubify
```

---

### 2. Install Python Requirements
Make sure you are using **Python 3.10+** (recommended 3.11).

```bash
pip install -r requirements.txt
```

If `requirements.txt` doesn't exist yet, install manually:

```bash
pip install streamlit fastapi uvicorn websockets faiss-cpu numpy sentence-transformers python-dotenv tqdm groq
```

---

### 3. Set Your Environment Variables

Create a **`.env`** file in the root folder:

```bash
touch .env
```

Inside `.env`, add:

```dotenv
GROQ_API_KEY=your_groq_api_key_here
```

> âš¡ **Important**: You must have a valid [Groq API Key](https://console.groq.com/keys) to access the LLM.

---

### 4. Build the FAISS Vectorstore
(Indexes your codebase into embeddings)

```bash
python -c "from utils.builder import build_vectorstore; build_vectorstore('path_to_your_codebase_folder', 'vectorstore')"
```

Example:

```bash
python -c "from utils.builder import build_vectorstore; build_vectorstore('BACKEND', 'vectorstore')"
```

This will create:
- `vectorstore/faiss_index.bin`
- `vectorstore/docs.pkl`

---

### 5. Start the FastAPI WebSocket Server
(Acts as the Main Control Plane - MCP)

```bash
uvicorn server.main:app --reload
```

> Server starts at `http://localhost:8000/ws`

---

## ğŸ§  Start the Agents

In separate terminals, run:

### 6. Retriever Agent
```bash
python agents/retriever_agent.py
```

### 7. Reasoner Agent
```bash
python agents/reasoner_agent.py
```

---

## ğŸ¯ Choose Your Client

You can use either the terminal or the Streamlit web interface.

---

### A. Terminal Client

```bash
python client/chat_client.py
```
- Text-based input and output.
- Simple, fast.

---

### B. Streamlit Web App Client

```bash
streamlit run streamlit_chat_client.py
```

- Modern chat UI (similar to ChatGPT).
- Keeps conversation history.
- Easy to use and interactive.

Access at: [http://localhost:8501](http://localhost:8501)

---

## ğŸ“š How the System Works

| Component            | Description |
|:---------------------|:------------|
| **Builder**           | Embeds your code files and stores them in FAISS vectorstore. |
| **Retriever Agent**   | Retrieves relevant code snippets based on your query. |
| **Reasoner Agent**    | Summarizes the retrieved code using a powerful LLM (Groq API). |
| **FastAPI Server**    | Orchestrates the communication between client and agents. |
| **Clients**           | Interface for users (Terminal or Web Streamlit). |

---

## ğŸ›  Troubleshooting

- **GROQ_API_KEY not working?**  
  â†’ Ensure you run `load_dotenv()` in your code, and your `.env` file is correct.

- **Vectorstore errors?**  
  â†’ Always run the `build_vectorstore()` step first.

- **WebSocket connection issues?**  
  â†’ Ensure FastAPI server and agents are running before the client.

- **Streamlit stuck or blank?**  
  â†’ Check the console for missing agent errors. Restart agents if necessary.

---

## âœ¨ Features

- Code-aware question answering
- Summarized and accurate code analysis
- Protects against hallucinated or unsafe answers
- Supports multi-language codebases (`.py`, `.js`, `.html`, `.css`, `.ts`, `.jsx`, `.tsx`, etc.)
- Modular agent-based architecture
- Beautiful Streamlit frontend

---

## ğŸ”¥ Upcoming Improvements (optional for future)

- Memory for long conversations
- Better UI using chat bubbles
- User authentication
- Multiple codebase support
- LLM selection (user can choose model)

---

# ğŸ“¢ Final Commands Quick Reference

```bash
# Build vectorstore (only once or after codebase updates)
python -c "from utils.builder import build_vectorstore; build_vectorstore('BACKEND', 'vectorstore')"

# Start server
uvicorn server.main:app --reload

# Start retriever agent
python agents/retriever_agent.py

# Start reasoner agent
python agents/reasoner_agent.py

# Start Terminal Client (optional)
python client/chat_client.py

# Start Streamlit Web Client (preferred)
streamlit run client/streamlit_chat_client.py
```

---

# â¤ï¸ Made with love for developers who love clean code.

---
